{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omddRuhBFlXN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {'fail': 0, 'pass': 1}\n",
        "    for label_str, label_num in label_map.items():\n",
        "        class_dir = os.path.join(data_dir, label_str)\n",
        "        print(f\"Searching in directory: {class_dir}\")\n",
        "        for root, _, files in os.walk(class_dir):\n",
        "            for fname in files:\n",
        "                img_path = os.path.join(root, fname)\n",
        "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "                    try:\n",
        "                        img = image.load_img(img_path, target_size=(150, 150))\n",
        "                        img_array = image.img_to_array(img)\n",
        "                        images.append(img_array)\n",
        "                        labels.append(label_num)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading {img_path}: {e}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
        "\n",
        "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        weight = alpha * tf.pow((1 - y_pred), gamma)\n",
        "        loss = weight * cross_entropy\n",
        "        return tf.reduce_mean(loss)\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# 경로 설정\n",
        "train_dir = '/content/drive/MyDrive/data/태림산업 이미지셋/Processed_Data_TUBE/iteration_1/train'\n",
        "test_dir = '/content/drive/MyDrive/data/태림산업 이미지셋/Processed_Data_TUBE/iteration_1/test'\n",
        "\n",
        "# 데이터 로드\n",
        "X_train, y_train = load_data(train_dir)\n",
        "X_test, y_test = load_data(test_dir)\n",
        "\n",
        "# 데이터 정규화\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# SMOTE를 사용한 데이터 증강\n",
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled_flat, y_resampled = smote.fit_resample(X_train_flat, y_train)\n",
        "X_resampled = X_resampled_flat.reshape((-1, 150, 150, 3))\n",
        "\n",
        "# CNN 모델 구성\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델 컴파일: 직접 구현한 Focal Loss 사용\n",
        "model.compile(\n",
        "    loss=focal_loss(gamma=2.0, alpha=0.25),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()\n",
        "\n",
        "# 클래스 가중치 설정\n",
        "class_weight = {0: 3.0, 1: 1.0}  # \"fail\" 클래스에 더 높은 가중치 부여\n",
        "\n",
        "# 모델 학습\n",
        "history = model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    batch_size=32,\n",
        "    epochs=60,  # 에포크 수 증가\n",
        "    validation_data=(X_test, y_test),\n",
        "    class_weight=class_weight  # 클래스 가중치 적용\n",
        ")\n",
        "\n",
        "# 모델 평가\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# 혼동행렬 계산 및 시각화\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# F1 스코어 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# 정확도 출력\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ]
}
