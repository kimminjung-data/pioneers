{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "locjxC9SnNAp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def load_data(data_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_map = {'fail': 0, 'pass': 1}\n",
        "    for label_str, label_num in label_map.items():\n",
        "        class_dir = os.path.join(data_dir, label_str)\n",
        "        print(f\"Searching in directory: {class_dir}\")\n",
        "        for root, _, files in os.walk(class_dir):\n",
        "            print(f\"Current directory: {root}, Number of files: {len(files)}\")\n",
        "            for fname in files:\n",
        "                img_path = os.path.join(root, fname)\n",
        "                if img_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):  # bmp 확장자 추가\n",
        "                    try:\n",
        "                        img = image.load_img(img_path, target_size=(150, 150))\n",
        "                        img_array = image.img_to_array(img)\n",
        "                        images.append(img_array)\n",
        "                        labels.append(label_num)\n",
        "                        print(f\"Loaded: {img_path}\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading {img_path}: {e}\")\n",
        "                else:\n",
        "                    print(f\"Skipped non-image file: {img_path}\")\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# 경로 설정\n",
        "train_dir = '/content/drive/MyDrive/data/태림산업 이미지셋/Processed_Data_TUBE/iteration_1/train'\n",
        "test_dir = '/content/drive/MyDrive/data/태림산업 이미지셋/Processed_Data_TUBE/iteration_1/test'\n",
        "\n",
        "# 데이터 로드\n",
        "X_train, y_train = load_data(train_dir)\n",
        "X_test, y_test = load_data(test_dir)\n",
        "\n",
        "\n",
        "\n",
        "# 데이터 정규화\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# SMOTE를 사용한 데이터 증강\n",
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))  # 2D로 변환\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled_flat, y_resampled = smote.fit_resample(X_train_flat, y_train)\n",
        "X_resampled = X_resampled_flat.reshape((-1, 150, 150, 3))  # 원래 이미지 형태로 복원\n",
        "\n",
        "# CNN 모델 구성\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # 이진 분류를 위한 시그모이드 활성화 함수\n",
        "])\n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# 현재 클래스가 0이 \"fail\"이고 1이 \"pass\"인 것으로 가정\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# \"fail\" 클래스에 더 높은 가중치를 주기 위해 수동으로 가중치 설정\n",
        "class_weight_dict = {0: 2.0, 1: 1.0}  # \"fail\" 클래스에 가중치 2.0을 부여\n",
        "\n",
        "# 모델 학습 (클래스 가중치 적용)\n",
        "history = model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    batch_size=32,\n",
        "    epochs=6,\n",
        "    validation_data=(X_test, y_test),\n",
        "    class_weight=class_weight_dict  # 클래스 가중치 적용\n",
        ")\n",
        "# 모델 평가\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# 혼동행렬 계산 및 시각화\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fail', 'Pass'], yticklabels=['Fail', 'Pass'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# F1 스코어 계산\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# 정확도 출력\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n"
      ]
    }
  ]
}